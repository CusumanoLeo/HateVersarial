{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fdf7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d78ec",
   "metadata": {},
   "source": [
    "### Using unicode database for homoglyphs\n",
    "https://www.unicode.org/reports/tr39/ :\n",
    "\n",
    "**Summary**\n",
    "Because Unicode contains such a large number of characters and incorporates the varied writing systems of the world, incorrect usage can expose programs or systems to possible security attacks. This document specifies mechanisms that can be used to detect possible security problems.\n",
    "\n",
    "**Status**\n",
    "This document has been reviewed by Unicode members and other interested parties, and has been approved for publication by the Unicode Consortium. This is a stable document and may be used as reference material or cited as a normative reference by other specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a433956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_homoglyph_map():\n",
    "    url = \"https://www.unicode.org/Public/security/latest/confusables.txt\" \n",
    "    response = requests.get(url) # Fetch the confusables data\n",
    "    raw_text = response.text     # Get the text content\n",
    "\n",
    "    homoglyph_map = defaultdict(list) \n",
    "\n",
    "    for line in raw_text.splitlines():\n",
    "        if line.startswith('#') or not line.strip(): # Skip comments and empty lines\n",
    "            continue\n",
    "        try:\n",
    "            src_hex, target_hex, *_ = line.split(';') # \n",
    "            src_char = chr(int(src_hex.strip(), 16))\n",
    "            target_chars = ''.join([chr(int(h, 16)) for h in target_hex.strip().split()])\n",
    "\n",
    "            # We only want visually similar substitutions that map to 1 character\n",
    "            if len(src_char) == 1 and len(target_chars) == 1:\n",
    "                ascii_base = target_chars.lower()\n",
    "                if ascii_base.isascii() and ascii_base.isalnum():\n",
    "                    homoglyph_map[ascii_base].append(src_char)\n",
    "        except Exception as e:\n",
    "            continue  # skip malformed lines\n",
    "\n",
    "    # Convert defaultdict to normal dict and deduplicate entries\n",
    "    homoglyph_map = {k: list(set(v)) for k, v in homoglyph_map.items()}\n",
    "\n",
    "    return homoglyph_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cbb72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a â†’ ['ğ”', 'ğ—”', 'ğš¨', 'Ğ°', 'ğ”„', 'ğ˜¼', 'ğ˜ˆ', 'ğ›‚', 'Î‘', 'ğŠ ', 'ğ“', 'ğ–†', 'Ğ', 'ğ•¬', 'ğ€', 'ğª', 'ğ–º', 'ğ', 'ğ™–', 'ğ‘¨', 'ğ’¶', 'ğ– ', 'ğš', 'ê“®', 'É‘', 'âº', 'ğ–½€', 'ğ™°', 'ğ’œ', 'ğ’‚', 'á—…', 'ğ•’', 'ï¼¡', 'ğ“ª', 'ğšŠ', '\\U0001ccd6', 'ğœ¶', 'ğ—®', 'ï½', 'ğ›¢', 'ğ°', 'ğ–', 'Î±', 'ğ”¸', 'áª', 'ğ˜¢', 'ğ›¼', 'ğ´', 'ğ‘', 'ğœœ']\n",
      "e â†’ ['ğ›¦', 'â„¯', 'ğ—²', 'ğ•°', 'ğ‘’', 'ğ–¤', 'â´¹', 'Î•', 'ğ‘¢®', 'ğ‘¬', 'â„®', 'ğ™´', 'â‹¿', 'ğ–¾', 'ğ—˜', 'ğœ ', 'ï¼¥', 'ğ”¼', 'ğ“®', 'ê¬²', 'ğ”¢', 'ğ“”', 'Ğ•', 'ğ”ˆ', 'ğ˜¦', 'ï½…', 'ğ‘¢¦', 'ğš¬', 'ğš', 'â…‡', 'ğ¸', 'ğ™€', 'ğ–Š', 'ğ„', 'ğ', 'ğŠ†', 'ğ”', '\\U0001ccda', 'ğ˜Œ', 'Ğµ', 'â„°', 'ğš', 'ğ’†', 'ğ•–', 'ê“°', 'á¬', 'Ò½', 'ğ™š']\n",
      "i â†’ ['ğ—¶', 'ï½‰', 'Î¹', 'ê™‡', 'ğœ¾', 'Ë›', 'ğ¢', 'Ó', 'ğ“²', 'ğ’¾', 'ğ™', 'ğ˜ª', 'á¾¾', 'Ñ–', 'ğ—‚', 'ğ‘–', 'ğ’Š', 'ğš¤', 'â³', 'É©', 'Ä±', 'Éª', 'ğ•š', 'ğ‘£ƒ', 'ğ–', 'ğš’', 'ğ²', 'ğ”¦', 'ê­µ', 'á¥', 'â„¹', 'ğ›Š', 'â…°', 'ğœ„', 'â…ˆ', 'ğ¸', 'Íº']\n",
      "o â†’ ['à¥¦', 'ğš¶', 'ğ˜–', 'â²Ÿ', 'ï®ª', 'ğ„', 'ï»¬', 'ğº„', 'ğœª', 'ğŠ’', 'à±¦', 'ğ™¾', 'ï»ª', 'à»', 'Ğ', 'Û•', 'ï®§', 'Ûµ', 'ğ—¢', 'Ğ¾', 'ğ“ª', 'à¹', 'ğ• ', 'ğ›°', 'ğ˜°', 'à²‚', 'ğ‘£—', 'ê¬½', 'à´ ', 'ğ¤', 'Õ•', 'ğ•º', 'ï»©', 'à³¦', 'ï®«', '0', 'ğ‘£ˆ', 'ğ¨', 'ğ“‚', 'ğŸ˜', 'ğŸ¯°', 'à°‚', 'á´‘', 'ğ', 'ğ’', 'ğ‘¶', 'ğ›”', 'ğ—¼', '\\U0001ccf0', 'à¶‚', 'ğ“¸', 'ğ›', 'ï®¦', 'ğŸ', 'ğˆ', 'ğŸ¬', 'ğ—ˆ', 'á€', 'ğ‘“', 'ğ¬', 'Ö…', 'ğ¾', 'ï½', 'à­¦', 'à§¦', 'ê“³', 'ğš˜', 'Û', 'ğ’ª', 'ğ‘£ ', 'áƒ¿', 'ğ™Š', 'ğŸ¶', 'à©¦', 'ğŠ«', 'ğ“', 'ğ¼', 'ÎŸ', 'àµ¦', '\\U0001cce4', 'ß€', 'á‹', 'à´‚', 'Ïƒ', 'Ù‡', 'ï»«', 'ğŸ¢', 'ğ¹¤', 'à«¦', 'ğ–®', 'ï®¨', 'ğœ', 'ğ”¬', 'ğ‘œ', 'ğ¸¤', 'ğ–”', 'ğ‚', 'ğ•†', 'â„´', 'âµ”', 'à¬ ', 'ã€‡', 'ï®¬', 'à¯¦', '×¡', 'ï®­', 'â²', 'ğœŠ', 'ï¼¯', 'á´', 'ğ”’', 'ğ‘¢µ', 'ğ”–', 'ğ„', 'ğ‘‚', 'ğ™¤', 'Ù¥', 'Î¿', 'ï®©', 'á€', 'ğ', 'Ú¾', 'ğ¸']\n",
      "s â†’ ['ğ‘†', 'áš', 'Ğ…', 'ğ•Š', 'ğ˜€', 'ğ”°', 'ğ’”', 'ğ“¼', 'Ñ•', 'ğŠ–', 'ğ‘º', 'ğ”–', 'ğ“¢', 'ğ˜š', 'ğ‘£', '\\U0001cce8', 'ğ ', 'ğ–˜', 'ğ’', 'á•', 'ğ‘ ', 'ğ–²', 'Õ', 'ğšœ', 'ğ‘ˆ', 'ê“¢', 'ğ•¾', 'êœ±', 'ğ—Œ', 'ğ˜´', 'ğ™¨', 'ğ’®', 'ğ™', 'ğš‚', 'ğ¬', 'ğ“ˆ', 'ê®ª', 'ğ–¼º', 'ğ•¤', 'ï½“', 'ï¼³', 'ğ—¦', 'Æ½']\n",
      "t â†’ ['ğ•¿', 'ê“”', 'ğ­', 'ğ‘»', 'Î¤', 'ğ™', 'ğœ¯', 'ğ˜µ', 'ğ—§', 'ğ—', 'ğ–³', 'ğŠ±', 'ğŠ—', 'ğ•‹', 'ğ‘‡', 'ğ“‰', 'ğ˜', 'ğ©', 'ğ‘¡', 'ï¼´', 'ğ’•', 'ğ‘¢¼', 'ğ”±', 'Ğ¢', 'ğ˜›', 'ğš', 'ğšƒ', 'ğ“', 'â²¦', 'ğ£', 'ğ“£', 'ğš»', 'ğŒ•', 'ğ–™', 'ğ–¼Š', 'ğ’¯', 'ğ›µ', 'ğŸ¨', 'âŸ™', 'ğ™©', '\\U0001cce9', 'âŠ¤', 'á¢', 'ğ”—', 'ğ•¥', 'ğ“½']\n"
     ]
    }
   ],
   "source": [
    "# Some letters to test\n",
    "if __name__ == \"__main__\":\n",
    "    homoglyph_map = build_homoglyph_map()\n",
    "    for letter in ['a', 'e', 'i', 'o', 's', 't']:\n",
    "        print(f\"{letter} â†’ {homoglyph_map.get(letter, [])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c57e281",
   "metadata": {},
   "source": [
    "### Leetspeak perturbations\n",
    "\n",
    "https://pypi.org/project/pyleetspeak/\n",
    "\n",
    "This tool aims to counter new misinformation that emerges in social media platforms by providing a mechanism for simulating and generating leetspeak/word camouflaging data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyleetspeak\n",
    "# ! pip install pyphen\n",
    "# ! pip install keybert\n",
    "# ! pip install codetiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668f19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyleetspeak.LeetSpeaker import LeetSpeaker\n",
    "from pyleetspeak.Leet_NER_generator import NER_data_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48abc650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I sp34k l3etsp3@k\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text_in = \"I speak leetspeak\"\n",
    "leeter = LeetSpeaker(\n",
    "    change_prb=0.8, change_frq=0.6, mode=\"basic\", seed=None, verbose=False\n",
    ")\n",
    "leet_result = leeter.text2leet(text_in)\n",
    "print(leet_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ce8543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hate', 'hat3', 'h@te', 'h4t3', 'h4te', 'h@t3']\n"
     ]
    }
   ],
   "source": [
    "# All possible combinations\n",
    "leeter = LeetSpeaker(get_all_combs=True, mode=\"basic\")\n",
    "combinations = leeter.text2leet(\"hate\")\n",
    "print(combinations)  # list of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f74851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean:     I hate you\n",
      "Augmented: 1 hat3 y0_\n",
      "---\n",
      "Clean:     Go back to your country\n",
      "Augmented: Go b@ck to your country\n",
      "---\n",
      "Clean:     You are disgusting\n",
      "Augmented: You @r3 d1sgust1ng\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "leeter = LeetSpeaker(\n",
    "    change_prb=0.6,     # probability of changing each char\n",
    "    change_frq=0.6,     # frequency across the whole string\n",
    "    mode=\"basic\",       # you can try \"intermediate\" or \"advanced\" too\n",
    "    seed=42,            # set for reproducibility\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"I hate you\",\n",
    "    \"Go back to your country\",\n",
    "    \"You are disgusting\"\n",
    "]\n",
    "\n",
    "augmented_texts = [leeter.text2leet(t) for t in texts]\n",
    "\n",
    "for clean, aug in zip(texts, augmented_texts):\n",
    "    print(f\"Clean:     {clean}\")\n",
    "    print(f\"Augmented: {aug}\")\n",
    "    print(\"---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a4a9dc",
   "metadata": {},
   "source": [
    "### Combine homoglyph and leetspeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "032c3e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def homoglyph_augment(text, homoglyph_map, prob=0.3):\n",
    "    new_text = \"\"\n",
    "    for char in text:\n",
    "        lower_char = char.lower()\n",
    "        if lower_char in homoglyph_map and random.random() < prob:\n",
    "            replacement = random.choice(homoglyph_map[lower_char])\n",
    "            new_text += replacement.upper() if char.isupper() else replacement\n",
    "        else:\n",
    "            new_text += char\n",
    "    return new_text\n",
    "\n",
    "def augment_text(text, leeter, homoglyph_map, leet_prob=0.5, homoglyph_prob=0.5):\n",
    "    aug_text = text\n",
    "\n",
    "    if random.random() < leet_prob:\n",
    "        aug_text = leeter.text2leet(aug_text)\n",
    "\n",
    "    if random.random() < homoglyph_prob:\n",
    "        aug_text = homoglyph_augment(aug_text, homoglyph_map)\n",
    "\n",
    "    return aug_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8920711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your clean dataset\n",
    "data = [\n",
    "    (\"I hate immigrants\", 1),\n",
    "    (\"Have a nice day\", 0),\n",
    "    (\"Go back to your country\", 1),\n",
    "    (\"Welcome to our community\", 0)\n",
    "]\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "for text, label in data:\n",
    "    aug_text = augment_text(text, leeter, homoglyph_map, leet_prob=0.7, homoglyph_prob=0.4)\n",
    "    augmented_data.append((aug_text, label))\n",
    "\n",
    "# Optionally merge with original\n",
    "full_dataset = data + augmented_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "460bb8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I hate immigrants', 1),\n",
       " ('Have a nice day', 0),\n",
       " ('Go back to your country', 1),\n",
       " ('Welcome to our community', 0),\n",
       " ('I h4t3 â³\\U0001cce2migr@nts', 1),\n",
       " ('Haâ…¤3 a n1c3 ğ”¡Ğy', 0),\n",
       " ('G0 ğµ@ck t0 y0ur ğ‚ouï¼®ğ’¯ry', 1),\n",
       " ('Welc0me t0 0ur community', 0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb55b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
    "            'labels': label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba2fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "226bdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `full_dataset` is already prepared (clean + leetspeak + homoglyph)\n",
    "dataset = HateSpeechDataset(full_dataset, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f091062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n",
      "torch.Size([8, 128])\n",
      "tensor([0, 1, 1, 0, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['input_ids'].shape)        # torch.Size([8, 128])\n",
    "    print(batch['attention_mask'].shape)   # torch.Size([8, 128])\n",
    "    print(batch['labels'])                 # Tensor of size [8]\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
